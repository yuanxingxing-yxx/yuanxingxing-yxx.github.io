[{"title":"ubuntu搭建jupyter notebook以及jupyter与spark的链接","date":"2020-05-25T15:48:27.000Z","path":"2020/05/25/ubuntu搭建jupyter notebook以及jupyter与spark的链接/","text":"搭建jupyter notebook安装python3更新软件包 1sudo apt-get update 安装 python3，默认 python3 将安装最新版本，一般Ubuntu都自带python在 /usr/local目录下 1sudo apt-get install python3 安装python第三方安装工具： 1sudo apt-get install python3-pip 查看python3版本信息 1python3 -V 安装jupyter notebook下载jupyter notebook 1pip3 install jupyter -i https://pypi.tuna.tsinghua.edu.cn/simple 运行 jupyter notebook 1jupyter notebook 使用find命令找出 jupyter 1find -name jupyter 将上述路径添加到环境变件中 1sudo gedit ~/.bashrc 1export PATH=~/.local/bin:$&#123;PATH&#125; 使环境变量生效 1source ~/.bashrc 运行 jupyter notebook 1jupyter notebook Jupyter Notebook是基于网页的用于交互计算的应用程序。可以直接在网页上新建文件进行编写 jupyter与spark的链接修改配置文件运行pyspark 12cd /usr/local/spark/bin./pyspark 直接运行pyspark可能会出现以下错误：因为没有配置Spark python的环境变量 12pyspark:行 45: python: 未找到命令 env: &quot;python&quot;: 没有那个文件或目录 解决办法：添加python相关环境变量 1gedit ~/.bashrc 12export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.7-src.zip:$PYTHONPATHexport PYSPARK_PYTHON=python3 注意：py4j-0.10.7-src.zip要到/usr/local/spark/python/lib目录查看是否是这个名称。不同版本的py4j的名称会有差别。 保存后，让环境变量生效 1source ~/.bashrc 再次运行pyspark 1./pyspark 退出pyspark 1exit() 链接jupyter与spark安装 findspark 1pip3 install findspark 测试 jupyter 是否成功连接 spark 123456import findsparkfindspark.init(&quot;/usr/local/spark&quot;) # 指明SPARK_HOMEimport pysparkfrom pyspark import SparkContext, SparkConfprint(&quot;hello spark&quot;) 成功链接，则会输出 “hello spark”至此，ubuntu搭建jupyter notebook以及jupyter与spark的链接就结束了，如果博客中有问题，欢迎各位大神们指点迷津","tags":[]},{"title":"Scala语言搭建编程环境","date":"2020-05-25T14:59:18.000Z","path":"2020/05/25/Scala语言搭建编程环境/","text":"sbt的下载与配置sbt的下载sbt官网下载链接: https://www.scala-sbt.org/download.html.https://www.scala-sbt.org/download.html选择sbt的版本，下载的文件会被默认保存在”/home/hadoop/下载”目录中。下载完成后，下载目录下便会有相应的压缩文件 sbt的配置将压缩包解压到/usr/local目录下，会自动生成名为sbt的文件夹 1sudo tar -zxf ~/下载/sbt-1.3.0.tgz -C /usr/local 为该文件夹授予权限 1sudo chown -R hadoop /usr/local/sbt 将bin目录下的sbt-launch.jar文件和bin目录下的sbt文件复制到当前目录下（当然也可以自己手动复制啦） 12cp ./bin/sbt-launch.jar ./cp ./bin/sbt ./ 在sbt脚本文件中，添加以下内容 123#!&#x2F;bin&#x2F;bashSBT_OPTS&#x3D;&quot;-Xms512M -Xmx1536M -Xss1M -XX:+CMSClassUnloadingEnabled -XX:MaxPermSize&#x3D;256M&quot;java $SBT_OPTS -jar &#96;dirname $0&#96;&#x2F;sbt-launch.jar &quot;$@&quot; 为sbt脚本文件增加可以执行权限 1chmod u+x ./sbt 运行如下命令，检验sbt是否可用确保计算机处于联网状态，首次运行该命令，会长时间处于“Getting org.scala-sbt sbt 1.2.8…” 的下载状态，要耐心等待。如果长时间没有进度（大概半小时左右），可能是网络问题导致安装失败，需要重新安装。 1./sbt sbt-version 至此，Scala语言搭建编程环境就结束了，如果博客中有问题，欢迎各位大神们指点迷津","tags":[]},{"title":"Spark计算环境的搭建","date":"2020-05-24T06:06:19.000Z","path":"2020/05/24/Spark计算环境的搭建/","text":"Spark的安装与配置Spark的下载Spark官网下载地址: http://spark.apache.org/downloads.html. 因为前期已经配置了Hadopp，所以在Choose a package type后面需要选择Pre-build with user-provided Hadoop，然后单击Download Spark后面的without-hadoop压缩包即可，下载的文件会被默认保存在”/home/hadoop/下载”目录中。选择推荐的镜像即可下载 1sudo tar -zxf ~/下载/spark-2.4.5-bin-without-hadoop.tgz -C /usr/local/ 为了方便目录的查看等，将解压后的文件重命名为spark 12cd /usr/localsudo mv ./spark-2.4.5-bin-without-hadoop/ ./spark 为文件授予权限，避免遇到文件无法创建等问题，注意更改为当前用户名 1sudo chown -R hadoop:hadoop ./spark # hadoop是当前登录Linux系统的用户名 Spark的配置修改Spark的配置文件spark-env.sh安装文件解压后，需要修改Spark的配置文件spark-env.sh，首先可以先复制一份Spark安装文件自带的配置文件模板，命令如下： 12cd /usr/local/sparkcp ./conf/spark-env.sh.template ./conf/spark-env.sh 使用gedit编辑器打开spark-env.sh文件进行编辑，在该文件的第一行添加一下配置信息： 1export SPARK_DIST_CLASSPATH=$(/usr/local/hadoop/bin/hadoop classpath) 验证Spark安装是否成功报错：/usr/local/spark/bin/spark-class: 行 71: /usr/lib/jdk/jdk1.8.0_221/bin/java: 没有那个文件或目录（详情可见小编的上一篇博客: here.）解决方式：jdk找不到路径问题，/usr/local/spark/conf/spark-env.sh文件中添加如下的 Java环境信息（可加到文本末尾，注意jdk版本号），直接配置修改文件spark-env中export （导入）jdk的路径即可。 1export JAVA_HOME=/usr/java/jdk1.8.0_221 有了上述的配置信息后，Spark就可以把数据存储到Hadoop分布式文件系统HDFS中，也可以从HDFS中读取数据。如果没有配置上面的信息，Spark就只能读写本地数据，无法读写HDFS中的数据。 配置完成后，就可以直接使用Spark，不需要像Hadoop那样运行启动命令，通过运行Spark自带的实例，可以验证Spark是否安装成功，命令如下： 12cd /usr/local/spark./bin/run-example SparkPi 执行时会输出很多屏幕信息，不容易找到最终的输出结果，为了从大量的输出信息中快速找到我们想要的执行结果，可以通过grep命令进行过滤。 1bin/run-example SparkPi 2&gt;&amp;1 | grep &quot;Pi is roughly&quot; 启动Spark shell 12cd /usr/local/spark./bin/spark-shell 在Spark shell中进行测试 12scala&gt; 7*7+8res0: Int = 57 关闭shell 1scala&gt;:quit 至此，Spark计算环境的搭建就结束了，如果博客中有问题，欢迎各位大神们指点迷津","tags":[]},{"title":"使用命令./bin/run-example SparkPi,验证Spark安装是否成功错-/usr/local/spark/bin/spark-class-行 71- /usr/lib/jdk/jdk1.8.0_221/bin/java-没有那个文件或目录","date":"2020-05-24T05:30:32.000Z","path":"2020/05/24/验证Spark安装是否成功报错__usr_lib_jdk_jdk1.8.0_221_bin_java_ 没有那个文件或目录/","text":"报错情况如下测试java是否安装成功 1java -version 检查java配置文件是否正确 1sudo gedit ~/.bashrc 检查该文件中对应的java路径是否正确 12345#set java envexport JAVA_HOME=/usr/lib/jdk/jdk1.8.0_221export JRE_HOME=$&#123;JAVA_HOME&#125;/jre export CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/lib export PATH=$&#123;JAVA_HOME&#125;/bin:$PATH 若不正确，则修改配置文件，并执行如下的命令使配置文件生效。（正确则忽略此步） 1source ~/.bashrc 配置软连接软连接相当于windows系统中的快捷键，部分软件可能会从/usr/bin目录下查找Java，因此添加该软连接防止其他软件查找不到的情况。 1sudo update-alternatives --install /usr/bin/java java /usr/java/jdk1.8.0_221/bin/java 300 1sudo update-alternatives --install /usr/bin/javac javac /usr/java/jdk1.8.0_221/bin/javac 300 再次尝试运行Spark自带的实例，若成功，则到此就结束，反之进行下一步。 配置修改文件spark-env中export （导入）jdk的路径jdk找不到路径问题，/usr/local/spark/conf/spark-env.sh文件中添加如下的 Java环境信息（可加到文本末尾，注意jdk版本号），直接配置修改文件spark-env中export （导入）jdk的路径即可。 1export JAVA_HOME=/usr/java/jdk1.8.0_221 再次进行验证 1./bin/run-example SparkPi 到此，该问题就解决啦，Spark可以正常使用了","tags":[]},{"title":"HBase Shell数据库操作","date":"2020-05-21T11:41:56.000Z","path":"2020/05/21/HBase Shell数据库操作/","text":"数据库表创建创建学生表（Student） 12./bin/hbase shellcreate &#x27;Student&#x27;,&#x27;S_ID&#x27;,&#x27;S_Name&#x27;,&#x27;S_Sex&#x27;,&#x27;S_Age&#x27; 创建课程表（Course） 1create &#x27;Course&#x27;,&#x27;C_ID&#x27;,&#x27;C_Name&#x27;,&#x27;C_Credit&#x27; 创建选课表（Selected_Course） 1create &#x27;Selected_Course&#x27;,&#x27;SC_ID&#x27;,&#x27;C_ID&#x27;,&#x27;SC_Score&#x27; 查看已创建的表 1list HBase Shell 数据访问操作学生表（Student）添加3条记录注意：每一行的数据，需要每一列添加，否则会报错显示学生表（Student）记录 1scan &#x27;Student&#x27; 课程表（Course）添加3条记录显示课程表（Course）记录 1scan &#x27;Course&#x27; 选课表（Selected_Course）添加4条记录显示选课表（Selected_Course）记录 1scan &#x27;Selected_Course&#x27; HBASE 数据访问程序设计HBase Shell命令显示学生表（Student）记录 1scan &#x27;Student&#x27; HBase Shell命令从课程表中查询（get）任意一条数据 1get &#x27;Course&#x27;,&#x27;c0001&#x27; 使用HBase Shell命令将选课表（Selected_Course）删除 123disable &#x27;Selected_Course&#x27;drop &#x27;Selected_Course&#x27;list HBase Shell数据库操作到此就结束了。如果博客中有问题，欢迎各位大神们指点迷津鸭。","tags":[]},{"title":"HDFS分布式文件系统的Java数据访问方法","date":"2020-05-16T06:40:33.000Z","path":"2020/05/16/HDFS分布式文件系统的Java数据访问方法/","text":"准备工作Ubuntu下安装IDEA1.下载地址: https://www.jetbrains.com/idea/download/#section=linux.小编选择的版本是ideaIU-2019.3.3.tar.gz 2.查看下载首先切换到下载目录要注意该处的hadoop，为当前用户名汉化版代码如下： 1cd /home/hadoop/下载 未汉化版： 1cd /home/hadoop/Downloads 3.将压缩包解压到/opt目录下注意压缩包的版本号 1sudo tar -zxvf ideaIU-2019.3.3.tar.gz -C /opt 4.进入安装位置并启动IDEA打开相应IDEA的bin目录，idea.sh是IDEA的启动文件，我们可以通过终端运行idea.sh文件启动 IDEA 5.测试安装是否成功(1)新建工程在弹出新建工程的界面选择Java，接着选择SDK，一般默认即可，点击“Next”按钮，在弹出的选择创建项目的模板页面，不做任何操作，直接点击“Next”按钮。输入项目名称，点击Finish，就完成了创建新项目的工作，小编所建的项目名称为：project1。 (2)新建java class(3)测试运行若成功输出相应的结果，则证明安装成功。到此Ubuntu下安装IDEA就完成了 相应包的导入1.在idea新建一个Java Project，并import需要的Hadoop JAR包在弹出新建工程的界面选择Java，接着选择SDK，一般默认即可，点击“Next”按钮，在弹出的选择创建项目的模板页面，不做任何操作，直接点击“Next”按钮。输入项目名称，点击Finish，就完成了创建新项目的工作，小编所建的项目名称为：Java Project。2.添加jar包，和Eclipse一样，要给项目添加相关依赖包，否则会出错。点击Idea的File菜单，然后点击“Project Structure”菜单依次点击Modules和Dependencies，然后选择“+”号3.选择hadoop的包，我用得是hadoop3.2.1。把下面的依赖包都加入到工程中，否则会出现某个类找不到的错误。 12345(1) /usr/local/hadoop/share/hadoop/common目录下的hadoop-common-3.2.1.jar和hadoop-nfs-3.2.1.jar。(2) /usr/local/hadoop/share/hadoop/common/lib目录下的所有jar包。(3) /usr/local/hadoop/share/hadoop/hdfs目录下的hadoop-hdfs-3.2.1.jar和hadoop-hdfs-nfs-3.2.1.jar。(4) /usr/local/hadoop/share/hadoop/hdfs/lib目录下的所有jar包。(5) /usr/local/hadoop/share/hadoop/hdfs目录下的 hadoop-hdfs-client-3.2.1.jar包 若不添加(5)对应的hadoop-hdfs-client-3.2.1.jar包，可能会报如下错误：4.将导入的包apply 启动Hadoop 12cd /usr/local/hadoop./sbin/start-dfs.sh 查看进程 1jps 从本地拷贝文件到HDFS1.新建java class2.在当前用户的桌面创建一个1.txt,作为传输文件进行测试。 1234567891011121314151617181920212223242526import java.io.BufferedInputStream;import java.io.FileInputStream;import java.io.InputStream;import java.io.OutputStream;import java.net.URI;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.io.IOUtils;import org.apache.hadoop.fs.Path;public class FileCopyFromLocal&#123; public static void main(String[] args) &#123; String source = &quot;/home/hadoop/桌面/1.txt&quot;; String dest = &quot;hdfs://localhost:9000/user/hadoop/input/1.txt&quot;; try &#123; InputStream in = new BufferedInputStream(new FileInputStream(source)); Configuration conf = new Configuration(); FileSystem fs = FileSystem.get(URI.create(dest),conf); OutputStream out = fs.create(new Path(dest)); IOUtils.copyBytes(in,out,4096,true); System.out.println(&quot;success&quot;); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 警告信息可忽略 成功拷贝则运行输出success 判断HDFS目录中对应文件是否存在1.新建java class2.查看1.txt,作为传输文件是否存在 123456789101112131415161718192021222324import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;public class HDFSFileIfExist&#123; public static void main(String[] args) &#123; try&#123; String fileName = &quot;hdfs://localhost:9000/user/hadoop/input/1.txt&quot;; Configuration conf = new Configuration(); conf.set(&quot;fs.defaultFS&quot;,&quot;hdfs://localhost:9000&quot;); conf.set(&quot;fs.hdfs.impl&quot;,&quot;org.apache.hadoop.hdfs.DistributedFileSystem&quot;); FileSystem fs = FileSystem.get(conf); if(fs.exists(new Path(fileName)))&#123; System.out.println(&quot;exists&quot;); &#125; else &#123; System.out.println(&quot;not exists&quot;); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 存在则运行输出exists! 列出HDFS目录中相应文件信息1.新建java class2.查看/user/hadoop/input目录内容 1234567891011121314151617181920212223import java.net.URI;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileStatus;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;public class ListHDFSFiles &#123; public static void main(String[] args) &#123; String uri = &quot;hdfs://localhost:9000/user/hadoop/input&quot;; Configuration conf = new Configuration(); try &#123; FileSystem fs = FileSystem.get(new URI(uri), conf); Path path = new Path(uri); FileStatus[] status = fs.listStatus(path); for (FileStatus s : status) &#123; System.out.println(s.getPath().getName()); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 显示如下： 读取HDFS目录中相应文件内容1.新建java class2.查看input目录下1.txt内容 1234567891011121314151617181920import java.io.*;import java.net.*;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.io.IOUtils;import org.apache.hadoop.fs.FsUrlStreamHandlerFactory;public class ReadHDFSFileContents&#123; public static void main(String[] args) &#123; Configuration conf = new Configuration(); conf.set(&quot;fs.defaultFs&quot;,&quot;hdfs://localhost:9000&quot;); conf.set(&quot;fs.hdfs.impl&quot;,&quot;org.apache.hadoop.hdfs.DistributedFileSystem&quot;); try &#123; URL.setURLStreamHandlerFactory(new FsUrlStreamHandlerFactory()); InputStream in = new URL(&quot;hdfs://localhost:9000/user/hadoop/input/1.txt&quot;).openStream(); IOUtils.copyBytes(in,System.out,4096,true); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 输出文件内容 读取HDFS相应文件的BLOCK信息1.新建java class2.查看input目录下1.txt所对应的block信息 123456789101112131415161718192021222324252627import java.net.URI;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileStatus;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.fs.BlockLocation;public class LocationFile&#123; public static void main(String[] args) &#123; String uri = &quot;hdfs://localhost:9000/user/hadoop/input/1.txt&quot;; Configuration conf = new Configuration(); try&#123; FileSystem fs = FileSystem.get(new URI(uri),conf); Path path = new Path(uri); FileStatus fileStatus = fs.getFileStatus(path); BlockLocation blockLocation[] = fs.getFileBlockLocations(fileStatus,0,fileStatus.getLen()); for(int i=0;i&lt;blockLocation.length;i++) &#123; String[] hosts = blockLocation[i].getHosts(); System.out.println(&quot;block_&quot;+i+&quot;_location:&quot;+hosts[0]); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 输出block信息则成功 删除HDFS目录中相应文件1.新建java class2.删除input目录下1.txt文件 1234567891011121314151617import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;public class DeleteHDFSFile&#123; public static void main(String[] args) &#123; Configuration conf = new Configuration(); conf.set(&quot;fs.defaultFS&quot;,&quot;hdfs://localhost:9000&quot;); try &#123; FileSystem fs = FileSystem.get(conf); boolean deleteOnExit = fs.deleteOnExit(new Path(&quot;/user/hadoop/input/1.txt&quot;)); System.out.println(deleteOnExit); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 删除成功则返回ture关闭hadoop 12./sbin/stop-dfs.shjps //查看进程 至此HDFS分布式文件系统的JAVA数据访问方法，就结束了。各位可爱们在实验过程中一定要注意细节哦，如果博客中有问题，欢迎各位大神们指点迷津。","tags":[]},{"title":"MapReduce分布式计算","date":"2020-05-14T13:35:58.000Z","path":"2020/05/14/MapReduce分布式计算实例(pi程序、wordcount程序、grep程序)/","text":"MapReduce分布式计算前期准备1.启动Hadoop 123cd /usr/local/hadoop./sbin/start-dfs.shjps 2.在桌面新建word.txt文件，并使用gedit编辑器在其中添加英文单词 123cd ./桌面touch word.txtgedit word.txt 3.在hdfs目录下创建input1和input2文件夹 1234./bin/hdfs dfs -ls./bin/hdfs dfs -mkdir -p /user/hadoop/input1./bin/hdfs dfs -mkdir -p /user/hadoop/input2./bin/hdfs dfs -ls 4.将桌面上的word.txt文件拷贝到input1目录下 1./bin/hdfs dfs -put /home/hadoop/桌面/word.txt input1 5.将/usr/local/hadoop/etc/hadoop目录下的所有.xml文件拷贝到input2目录下 1./bin/hdfs dfs -put ./etc/hadoop/*.xml input2 6.检查拷贝是否成功 12./bin/hdfs dfs -ls input1./bin/hdfs dfs -ls input2 MapReduce的圆周率计算方法1.查看mapreduce目录所在位置 1cd /usr/local/hadoop/share/hadoop/mapreduce 2.切换目录，通过使用hadoop-mapreduce-examples-3.2.1.jar包计算pi的值 12cd /usr/local/hadoop./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar pi 5 500 //pi + map数量 + reduce数量 3.查看相应的计算结果 MapReduce的Wordcount计算方法1.查看input1目录下word.txt文件内容 1./bin/hdfs dfs -cat input1/word.txt 2.通过使用hadoop-mapreduce-examples-3.2.1.jar包统计单词个数，并保存于output1目录下 1./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar wordcount input1 output1 3.查看MapReduce的Wordcount计算结果 123./bin/hdfs dfs -ls./bin/hdfs dfs -ls output1./bin/hdfs dfs -cat output1/part-r-00000 MapReduce的正则表达式匹配计算方法1.使用上传在input2目录中的XML文件，并使用MapReduce相应计算程序，完成正则表达式计算，统计该.xml文本中满足’dfs[a-z.]+ ’匹配规则的字符 1./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar grep input2 output2 &#x27;dfs[a-z.]+ &#x27; 2.查看output2目录是否创建成功 1./bin/hdfs dfs -ls 3.查看output2目录 1./bin/hdfs dfs -ls output2 4.查看MapReduce的grep正则表达式匹配计算结果 1./bin/hdfs dfs -cat output2/part-r-00000 关闭hdfs 1./sbin/stop-dfs.sh 本次MapReduce分布式计算实例：pi程序、wordcount程序、grep程序演示到此就结束了，各位可爱们在实验过程中一定要注意细节哦，如果博客中有问题，欢迎各位大神们指点迷津","tags":[]},{"title":"Ubuntu下搭建HBase的单机模式以及伪分布式模式","date":"2020-05-07T16:01:12.000Z","path":"2020/05/08/Ubuntu下搭建HBase的单机模式以及伪分布式模式/","text":"HBase环境的安装HBase压缩文件解压将下载文件夹中的HBase压缩文件解压(可选,Windows与ubuntu的文件共享可参考上一篇文章：Ubuntu下搭建Hadoop的单机模式以及伪分布式模式) 1sudo tar -xzvf ~/下载/hbase-2.2.0-bin.tar.gz 为了方便使用将hbase解压后的文件更名。 文件授予权限为文件授予权限，避免遇到文件无法创建等问题，注意更改为当前用户名 1sudo chown -R hadoop ./hbase HBase单机模式配置修改hbase-env.sh配置文件修改conf文件夹中hbase-env.sh配置文件 1gedit hbase-env.sh 因为hbase是基于Zookeeper进行协调管理，则删除下图中的‘#’删除此处‘#’，并查看java目录下的jdk版本 修改hbase-site.xml配置文件修改conf文件夹中hbase-site.xml配置文件 1gedit hbase-site.xml 验证HBase版本号切换到bin目录下，验证hbase版本号 1./hbase version 启动HBase数据库系统bin目录下，启动HBase数据库系统 1./start-hbase.sh jps查看进程jps查看进程，检查hbase是否启动，若包括HMaster则启动成功 1jps 启动HBase命令行模式启动HBase数据库命令行模式 1./hbase shell 启动成功标志如下 查看HBase命令行模式下的进程此时，打开另一个终端查看进程，main代表启动hbase终端，即可进行创建表等基本操作 关闭HBase命令行模式关闭HBase数据库命令行模式 1quit 关闭hbase数据库1./stop-hbase.sh HBase伪分布式配置配置hbase-env.sh文件修改conf目录下的hbase-env.sh文件使用gedit编辑器打开/usr/local/hbase/conf/hbase-env.sh，命令如下： 1gedit /usr/local/hbase/conf/hbase-env.sh 修改配置文件如下三处后保存 配置hbase-site.xml文件修改conf文件夹中hbase-site.xml配置文件 1gedit /usr/local/hbase/conf/hbase-site.xml 12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000/hbase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 启动运行HBase首先登陆ssh，由于搭建hadoop时已经设置了无密码登陆，因此这里不需要密码，然后切换到/usr/local/hadoop目录下，启动Hadoop，让HDFS进入运行状态，从而可以为HBase存储数据，具体命令如下： 123ssh localhostcd /usr/local/hadoop./sbin/start-dfs.sh 输入命令jps，如果能看到NameNode，DataNode，SecondNode这三个进程，则表示已经成功启动Hadoop然后启动HBase，命令如下 12cd /usr/local/hbase/bin./start-hbase.sh 启动HBase Shell模式进入HBase Shell模式，命令如下： 1./hbase shell 关闭HBase Shell模式1quit 关闭HBase在/usr/local/hbase/bin目录下，可以使用如下命令停止HBase运行： 1./stop-hbase.sh 12cd /usr/local/hadoop./sbin/stop-dfs.sh 如果在操作HBase的过程中发生错误，可以查看{HBASE_HOME}目录(即/usr/local/hbase)下的logs子目录中的日志文件，来寻找可能的错误原因。 最后需要注意的是，启动关闭Hadoop和HBase的顺序一定是：启动Hadoop-&gt;启动HBase-&gt;关闭HBase-&gt;关闭Hadoop。 Ubuntu下搭建HBase的单机模式以及伪分布式模式到此就结束了。小编经过疯狂的踩坑，熟悉了linux操作系统的部分命令。各位可爱们在搭建过程中一定要注意细节哦，如果博客中有问题，欢迎各位大神们指点迷津。","tags":[]},{"title":"Ubuntu下搭建Hadoop的单机模式以及伪分布式模式","date":"2020-04-24T11:20:21.000Z","path":"2020/04/24/Ubuntu下搭建Hadoop的单机模式以及伪分布式模式/","text":"环境准备创建Hadoop用户(可选)1.如果安装 Ubuntu 的时候不是用的 “hadoop” 用户，那么需要增加一个名为hadoop 的用户，首先按打开终端窗口（快捷键ctrl+alt+t），输入如下命令创建新用户 :创建hadoop用户。 1sudo useradd –m hadoop –s /bin/bash 2.上面这条命令创建了可以登陆的 hadoop 用户，并使用 /bin/bash 作为 shell 接着使用如下命令设置密码，可简单设置为 hadoop，按提示输入两次密码: 1sudo passwd hadoop 3.可为 hadoop 用户增加管理员权限，方便部署，避免一些对新手来说比较棘 手的权限问题: 1sudo adduser hadoop sudo SSH登录权限设置1.SSH简介SSH 为 Secure Shell 的缩写，是建立在应用层和传输层基础上的安全协议。 SSH 是目前较可靠、专为远程登录会话和其他网络服务提供安全性的协议。 利用 SSH 协议可以有效防止远程管理过程中的信息泄露问题。SSH最初是 UNIX系统上的一个程序，后来又迅速扩展到其他操作平台。 SSH是由客 户端和服务端的软件组成，服务端是一个守护进程(daemon)，它在后台运 行并响应来自客户端的连接请求，客户端包含ssh程序以及像scp(远程拷 贝)、slogin(远程登陆)、sftp(安全文件传输)等其他的应用程序。2. 配置SSH的原因Hadoop名称节点(NameNode)需要启动集群中所有机器的Hadoop守护进程，这个过 程需要通过SSH登录来实现。Hadoop并没有提供SSH输入密码登录的形式，因此，为 了能够顺利登录每台机器，需要将所有机器配置为名称节点可以无密码登录它们。3. 切换登录用户以hadoop登录（可选）4. 更新（可选） 1sudo apt-get update 5.（1） 配置SSH的无密码登录安装openssh-server( 通常Linux系统会默认安装openssh的客户端软件openssh-client)，所以需要自己安装一下服务端。 1sudo apt-get install openssh-server （2）输入 cd .ssh目录下，如果没有.ssh文件 输入 ssh localhost生成 1cd ~/.ssh/ （3）生成秘钥 1ssh-keygen -t rsa （4）将Master中生成的密钥加入授权（authorized_keys） 1cat id_rsa.pub # 查看生成的公钥 12cat id_rsa.pub &gt;&gt; authorized_keys # 加入授权chmod 600 authorized_keys # 修改文件权限，如果不修改文件权限那么其它用户就能查看该授权 （5）完成后，直接键入“ssh localhost”，能无密码登录即可（6）键入“exit”退出，到此SSH无密码登录配置就成功了 安装Java环境1.Windows与Ubuntu数据共享（可将Windows下的文件进行共享，可选）（1）Windows系统操作（2）Ubuntu系统操作（基于VirtualBox）点击设备-&gt;安装增强功能输入密码后直接回车再次点击设备-&gt;共享文件夹-&gt;共享文件夹桌面会增加两个盘，点击sf_盘输入密码，即可将改文件夹中的内容复制到ubuntu下（小编复制到了下载文件夹中） 移动完成后，即可删除两个盘 2.在Ubuntu将jdk移动到我们新建的java目录下（没建的新建一个就是），到此传输文件成功，可以开始配置Java环境了。 1sudo mkdir java 注意根据自己的jdk版本号以及当前用户名执行 1sudo mv /home/hadoop/下载/jdk-8u221-linux-x64.tar.gz usr/java 在java目录中，使用sudo tar命令解压jdk文件，解压成功后，java目录中会有对应的目录文件存在 1sudo tar -zxvf jdk-8u221-linux-x64.tar.gz 3.配置java环境（1） 使用命令“sudo gedit ~/.bashrc”打开配置文件，在末尾添加以下几行文字，注意自己的jdk版本号。 12345 #set java envexport JAVA_HOME=/usr/lib/jdk/jdk1.8.0_221export JRE_HOME=$&#123;JAVA_HOME&#125;/jre export CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/lib export PATH=$&#123;JAVA_HOME&#125;/bin:$PATH（2）使用命令“source ~/.bashrc”使环境变量生效（关闭配置文件并在当前目录即/usr/java目录下执行）。 1source ~/.bashrc （3）配置软连接，软连接相当于windows系统中的快捷键，部分软件可能会从/usr/bin目录下查找Java，因此添加该软连接防止其他软件查找不到的情况。 1sudo update-alternatives --install /usr/bin/java java /usr/java/jdk1.8.0_221/bin/java 300 1sudo update-alternatives --install /usr/bin/javac javac /usr/java/jdk1.8.0_221/bin/javac 300 （4）测试java是否安装成功 1java -version 单机模式以及伪分布式模式的搭建Hadoop单机安装配置1.将我们下载的Hadoop解压到 /usr/local/ 中（与解压jdk类似） 1sudo tar zxvf /home/hadoop/下载/hadoop-3.2.1.tar.gz -C /usr/local 2.利用cd /usr/local/ 命令切换操作空间，将文件夹名改为hadoop 1sudo mv ./hadoop-3.2.1/ ./hadoop 3.修改文件权限 1sudo chown -R hadoop:hadoop ./hadoop 1sudo chown -R 当前用户名 /usr/local/hadoop 4.修改配置文件Hadoop 解压后，在hadoop目录下的etc/hadoop/hadoop-env.sh文件中添加如下的 Java环境信息（可加到文本末尾，注意jdk版本号） 1export JAVA_HOME=/usr/java/jdk1.8.0_221 5.然后，保存hadoop-env.sh文件，即完成单机模式的Hadoop基本安装。测试Hadoop是否安装成功，如出现如下图所示的版本信息，即可。 1./bin/hadoop/ version 6.默认情况下，单机模式的Hadoop以Java进程的方式运行，可依次运行如下命令进行进一步测试。 12sudo mkdir inputsudo cp etc/hadoop/*.xml input 7.执行下列命令，运行MapReduce程序，完成测试计算。 1bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar grep input output &#x27;dfs[a-z.]+&#x27; 8. 执行下列命令，查看计算结果。 1cat output/* hadoop目录下，会有input和output两个新建的文件，output中有上述程序 的运算结果，到此hadoop单机安装配置成功。 Hadoop伪分布式安装配置1.Hadoop伪分布式安装配置（1）Hadoop可以在单节点上以伪分布式的方式运行，Hadoop进程以分 离的 Java 进程来运行，节点既作为 NameNode 也作为 DataNode， 同时，读取的是 HDFS 中的文件（2）Hadoop的配置文件位于/usr/local/hadoop/etc/hadoop/中，伪分布式 需要修改2个配置文件 core-site.xml 和 hdfs-site.xml（3）Hadoop的配置文件是xml格式，每个配置以声明property的name 和 value 的方式来实现hadoop目录认识2.hadoop下的目录（1）修改配置文件之前，先看一下hadoop下的目录：bin：hadoop最基本的管理脚本和使用脚本所在目录，这些脚本是sbin目录下管理脚本的基础实现，用户可以直接使用这些脚本管理和使用hadoop（2）etc：配置文件存放的目录，包括core-site.xml,hdfs-site.xml,mapred-site.xml等从hadoop1.x继承而来的配置文件和yarn-site.xml等hadoop2.x新增的配置文件（3）include：对外提供的编程库头文件（具体动态库和静态库在lib目录中，这些头文件军事用c++定义的，通常用于c++程序访问hdfs或者编写mapreduce程序）（4）Lib：该目录包含了hadoop对外提供的才变成动态库和静态库，与include目录中的头文件结合使用（5）libexec：各个服务对应的shell配置文件所在目录，可用于配置日志输出目录、启动参数等信息（6）sbin：hadoop管理脚本所在目录，主要包含hdfs和yarn中各类服务的启动、关闭脚本（7）share：hadoop各个模块编译后的jar包所在目录。3. 修改配置文件 core-site.xml 1234567891011 &lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt; &lt;description&gt;Abase for other temporary directories.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; （1）hadoop.tmp.dir表示存放临时数据的目录，即包括NameNode的数据，也包 括DataNode的数据。该路径任意指定，只要实际存在该文件夹即可（2）name为fs.defaultFS的值，表示hdfs路径的逻辑名称 4.修改配置文件 hdfs-site.xml 1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/data&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; （1）dfs.replication表示副本的数量，伪分布式要设置为1（2）dfs.namenode.name.dir表示本地磁盘目录，是存储fsimage文件的地方（3）dfs.datanode.data.dir表示本地磁盘目录，HDFS数据存放block的地方 5.至此，配置完毕，但是还不能启动，要对hdfs先进行格式化。类似以前的软盘，使用前要先格式化,执行如下命令，看到日志信息，即格式化成功。 1sudo ./bin/hdfs namenode -format 6.在我们name目录(这个目录是我们自己配置的时候指定的)下也会出现映像文件（fsimage），用于将数据持久化 。7.启动hadoop 1sbin/start-dfs.sh 8.安装jps 1sudo apt install openjdk-11-jdk-headless 9.安装好之后jps检查角色如果有多个角色，就启动成功。 1jps 10.浏览器访问localhost:9870 11.关闭hadoop（使用完毕后一定要关闭，否则相当容易崩） 1./sbin/stop-dfs.sh Ubuntu下搭建Hadoop的单机模式以及伪分布式模式到此就结束了。小编经过疯狂的踩坑，熟悉了linux操作系统的部分命令。各位可爱们在搭建过程中一定要注意细节哦，配置文件这些可以不用自己手敲，通过复制代码，保存在文本文件中，再利用Windows和Ubuntu数据共享，很快就可以节省时间啦。最后，如果博客中有问题，希望各位大神们指点迷津。","tags":[]}]